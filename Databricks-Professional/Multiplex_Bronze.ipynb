{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1267ce9-3e0d-4649-a462-4d3aba6ea63d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def path_exists(path):\n",
    "  try:\n",
    "    dbutils.fs.ls(path)\n",
    "    return True\n",
    "  except Exception as e:\n",
    "    if 'java.io.FileNotFoundException' in str(e):\n",
    "      return False\n",
    "    else:\n",
    "      raise\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "class CourseDataset:\n",
    "    def __init__(self, uri, location, checkpoint_path, data_catalog, db_name):\n",
    "        self.uri = uri\n",
    "        self.location = location\n",
    "        self.checkpoint = checkpoint_path\n",
    "        self.catalog_name = data_catalog\n",
    "        self.db_name = db_name\n",
    "    \n",
    "    def download_dataset(self):\n",
    "        source = self.uri\n",
    "        target = self.location\n",
    "        files = dbutils.fs.ls(source)\n",
    "\n",
    "        for f in files:\n",
    "            source_path = f\"{source}/{f.name}\"\n",
    "            target_path = f\"{target}/{f.name}\"\n",
    "            if not path_exists(target_path):\n",
    "                print(f\"Copying {f.name} ...\")\n",
    "                dbutils.fs.cp(source_path, target_path, True)\n",
    "    \n",
    "    \n",
    "    def create_database(self):\n",
    "        spark.sql(f\"USE CATALOG {self.catalog_name}\")\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {self.db_name}\")\n",
    "        spark.sql(f\"USE SCHEMA {self.db_name}\")\n",
    "    \n",
    "    \n",
    "    def clean_up(self):\n",
    "        print(\"Removing Checkpoints ...\")\n",
    "        dbutils.fs.rm(self.checkpoint, True)\n",
    "        print(\"Dropping Database ...\")\n",
    "        spark.sql(f\"DROP SCHEMA IF EXISTS {self.db_name} CASCADE\")\n",
    "        print(\"Removing Dataset ...\")\n",
    "        dbutils.fs.rm(self.location, True)\n",
    "        print(\"Done\")\n",
    "\n",
    "    \n",
    "    def __get_index(self, dir):\n",
    "        try:\n",
    "            files = dbutils.fs.ls(dir)\n",
    "            file = max(f.name for f in files if f.name.endswith('.json'))\n",
    "            index = int(file.rsplit('.', maxsplit=1)[0])\n",
    "        except:\n",
    "            index = 0\n",
    "        return index+1\n",
    "    \n",
    "    \n",
    "    def __load_json_file(self, current_index, streaming_dir, raw_dir):\n",
    "        latest_file = f\"{str(current_index).zfill(2)}.json\"\n",
    "        source = f\"{streaming_dir}/{latest_file}\"\n",
    "        target = f\"{raw_dir}/{latest_file}\"\n",
    "        prefix = streaming_dir.split(\"/\")[-1]\n",
    "        if path_exists(source):\n",
    "            print(f\"Loading {prefix}-{latest_file} file to the bookstore dataset\")\n",
    "            dbutils.fs.cp(source, target)\n",
    "    \n",
    "    \n",
    "    def __load_data(self, max, streaming_dir, raw_dir, all=False):\n",
    "        index = self.__get_index(raw_dir)\n",
    "        if index > max:\n",
    "            print(\"No more data to load\\n\")\n",
    "\n",
    "        elif all == True:\n",
    "            while index <= max:\n",
    "                self.__load_json_file(index, streaming_dir, raw_dir)\n",
    "                index += 1\n",
    "        else:\n",
    "            self.__load_json_file(index, streaming_dir, raw_dir)\n",
    "            index += 1\n",
    "    \n",
    "    def load_new_data(self, num_files = 1):\n",
    "        streaming_dir = f\"{self.location}/kafka-streaming\"\n",
    "        raw_dir = f\"{self.location}/kafka-raw\"\n",
    "        for i in range(num_files):\n",
    "            self.__load_data(10, streaming_dir, raw_dir)\n",
    "        \n",
    "    \n",
    "    def load_books_updates(self):\n",
    "        streaming_dir = f\"{self.location}/books-updates-streaming\"\n",
    "        raw_dir = f\"{self.location}/kafka-raw/books-updates\"\n",
    "        self.__load_data(5, streaming_dir, raw_dir)\n",
    "        \n",
    "    def process_bronze(self):\n",
    "        schema = \"key BINARY, value BINARY, topic STRING, partition LONG, offset LONG, timestamp LONG\"\n",
    "\n",
    "        query = (spark.readStream\n",
    "                            .format(\"cloudFiles\")\n",
    "                            .option(\"cloudFiles.format\", \"json\")\n",
    "                            .schema(schema)\n",
    "                            .load(f\"{self.location}/kafka-raw\")\n",
    "                            .withColumn(\"timestamp\", (F.col(\"timestamp\")/1000).cast(\"timestamp\"))  \n",
    "                            .withColumn(\"year_month\", F.date_format(\"timestamp\", \"yyyy-MM\"))\n",
    "                      .writeStream\n",
    "                          .option(\"checkpointLocation\", f\"{self.checkpoint}/bronze\")\n",
    "                          .option(\"mergeSchema\", True)\n",
    "                          .partitionBy(\"topic\", \"year_month\")\n",
    "                          .trigger(availableNow=True)\n",
    "                          .table(\"bronze\"))\n",
    "\n",
    "        query.awaitTermination()\n",
    "        \n",
    "        \n",
    "    def __upsert_data(self, microBatchDF, batch):\n",
    "        microBatchDF.createOrReplaceTempView(\"orders_microbatch\")\n",
    "    \n",
    "        sql_query = \"\"\"\n",
    "          MERGE INTO orders_silver a\n",
    "          USING orders_microbatch b\n",
    "          ON a.order_id=b.order_id AND a.order_timestamp=b.order_timestamp\n",
    "          WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "\n",
    "        microBatchDF.sparkSession.sql(sql_query)\n",
    "        \n",
    "    def __batch_upsert(self, microBatchDF, batchId):\n",
    "        window = Window.partitionBy(\"customer_id\").orderBy(F.col(\"row_time\").desc())\n",
    "        \n",
    "        (microBatchDF.filter(F.col(\"row_status\").isin([\"insert\", \"update\"]))\n",
    "                     .withColumn(\"rank\", F.rank().over(window))\n",
    "                     .filter(\"rank == 1\")\n",
    "                     .drop(\"rank\")\n",
    "                     .createOrReplaceTempView(\"ranked_updates\"))\n",
    "\n",
    "        query = \"\"\"\n",
    "            MERGE INTO customers_silver c\n",
    "            USING ranked_updates r\n",
    "            ON c.customer_id=r.customer_id\n",
    "                WHEN MATCHED AND c.row_time < r.row_time\n",
    "                  THEN UPDATE SET *\n",
    "                WHEN NOT MATCHED\n",
    "                  THEN INSERT *\n",
    "        \"\"\"\n",
    "\n",
    "        microBatchDF.sparkSession.sql(query)\n",
    "        \n",
    "    \n",
    "    def __type2_upsert(self, microBatchDF, batch):\n",
    "        microBatchDF.createOrReplaceTempView(\"updates\")\n",
    "\n",
    "        sql_query = \"\"\"\n",
    "            MERGE INTO books_silver\n",
    "            USING (\n",
    "                SELECT updates.book_id as merge_key, updates.*\n",
    "                FROM updates\n",
    "\n",
    "                UNION ALL\n",
    "\n",
    "                SELECT NULL as merge_key, updates.*\n",
    "                FROM updates\n",
    "                JOIN books_silver ON updates.book_id = books_silver.book_id\n",
    "                WHERE books_silver.current = true AND updates.price <> books_silver.price\n",
    "              ) staged_updates\n",
    "            ON books_silver.book_id = merge_key \n",
    "            WHEN MATCHED AND books_silver.current = true AND books_silver.price <> staged_updates.price THEN\n",
    "              UPDATE SET current = false, end_date = staged_updates.updated\n",
    "            WHEN NOT MATCHED THEN\n",
    "              INSERT (book_id, title, author, price, current, effective_date, end_date)\n",
    "              VALUES (staged_updates.book_id, staged_updates.title, staged_updates.author, staged_updates.price, true, staged_updates.updated, NULL)\n",
    "        \"\"\"\n",
    "\n",
    "        microBatchDF.sparkSession.sql(sql_query)\n",
    "    \n",
    "    def process_orders_silver(self):\n",
    "        json_schema = \"order_id STRING, order_timestamp Timestamp, customer_id STRING, quantity BIGINT, total BIGINT, books ARRAY<STRUCT<book_id STRING, quantity BIGINT, subtotal BIGINT>>\"\n",
    "        \n",
    "        deduped_df = (spark.readStream\n",
    "                   .table(\"bronze\")\n",
    "                   .filter(\"topic = 'orders'\")\n",
    "                   .select(F.from_json(F.col(\"value\").cast(\"string\"), json_schema).alias(\"v\"))\n",
    "                   .select(\"v.*\")\n",
    "                   .withWatermark(\"order_timestamp\", \"30 seconds\")\n",
    "                   .dropDuplicates([\"order_id\", \"order_timestamp\"]))\n",
    "        \n",
    "        query = (deduped_df.writeStream\n",
    "                   .foreachBatch(self.__upsert_data)\n",
    "                   .outputMode(\"update\")\n",
    "                   .option(\"checkpointLocation\", f\"{self.checkpoint}/orders_silver\")\n",
    "                   .trigger(availableNow=True)\n",
    "                   .start())\n",
    "\n",
    "        query.awaitTermination()\n",
    "\n",
    "        \n",
    "    def process_customers_silver(self):\n",
    "        \n",
    "        schema = \"customer_id STRING, email STRING, first_name STRING, last_name STRING, gender STRING, street STRING, city STRING, country_code STRING, row_status STRING, row_time timestamp\"\n",
    "        \n",
    "        df_country_lookup = spark.read.json(f\"{dataset_bookstore}/country_lookup\")\n",
    "\n",
    "        query = (spark.readStream\n",
    "                          .table(\"bronze\")\n",
    "                          .filter(\"topic = 'customers'\")\n",
    "                          .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "                          .select(\"v.*\")\n",
    "                          .join(F.broadcast(df_country_lookup), F.col(\"country_code\") == F.col(\"code\") , \"inner\")\n",
    "                       .writeStream\n",
    "                          .foreachBatch(self.__batch_upsert)\n",
    "                          .outputMode(\"update\")\n",
    "                          .option(\"checkpointLocation\", f\"{self.checkpoint}/customers_silver\")\n",
    "                          .trigger(availableNow=True)\n",
    "                          .start()\n",
    "                )\n",
    "\n",
    "        query.awaitTermination()\n",
    "    \n",
    "    def process_books_silver(self):\n",
    "        schema = \"book_id STRING, title STRING, author STRING, price DOUBLE, updated TIMESTAMP\"\n",
    "\n",
    "        query = (spark.readStream\n",
    "                        .table(\"bronze\")\n",
    "                        .filter(\"topic = 'books'\")\n",
    "                        .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "                        .select(\"v.*\")\n",
    "                     .writeStream\n",
    "                        .foreachBatch(self.__type2_upsert)\n",
    "                        .option(\"checkpointLocation\", f\"{self.checkpoint}/books_silver\")\n",
    "                        .trigger(availableNow=True)\n",
    "                        .start()\n",
    "                )\n",
    "\n",
    "        query.awaitTermination()\n",
    "        \n",
    "    def process_current_books(self):\n",
    "        spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TABLE current_books\n",
    "            AS SELECT book_id, title, author, price\n",
    "               FROM books_silver\n",
    "               WHERE current IS TRUE\n",
    "        \"\"\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "data_source_uri = \"s3://dalhussein-courses/DE-Pro/datasets/bookstore/v1/\"\n",
    "dataset_bookstore = 'dbfs:/mnt/demo-datasets/DE-Pro/bookstore'\n",
    "spark.conf.set(f\"dataset.bookstore\", dataset_bookstore)\n",
    "checkpoint_path = \"dbfs:/mnt/demo_pro/checkpoints\"\n",
    "data_catalog = 'hive_metastore'\n",
    "db_name = \"bookstore_eng_pro\"\n",
    "\n",
    "bookstore = CourseDataset(data_source_uri, dataset_bookstore, checkpoint_path, data_catalog, db_name)\n",
    "bookstore.download_dataset()\n",
    "bookstore.create_database()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#bookstore.clean_up()\n",
    "\n",
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94261aac-97c0-4486-a05f-75efcfa6529d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls dbfs:/mnt/demo-datasets/DE-Pro/bookstore/kafka-raw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1560ecad-e235-44e0-a113-fb5f566664ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp, unix_timestamp, from_unixtime, current_date, date_format, lit, col, date_format, to_date\n",
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType, LongType, DoubleType\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efca91b5-7edd-4718-8cb4-c1038997560a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw = spark.read.json(\"dbfs:/mnt/demo-datasets/DE-Pro/bookstore/kafka-raw/01.json\")\n",
    "display(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bb8d3c8-5215-4dd4-8a07-b67f51b2c7d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae0962ef-c040-413f-b217-a9ea727c8cf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_df = df_raw.withColumn('current_timestamp', current_timestamp()) \\\n",
    "        .withColumn('current_date', current_date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05e53f53-befd-401e-af40-b770918926bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f7ad8b2-60b1-4de9-bcf0-f92f3712ef62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_df = new_df.withColumn(\"current_date\", date_format(col(\"current_date\"), \"yyyy.MM.dd\"))\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "742ec497-eeeb-4b52-9805-5e9b8d3551d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_df = new_df.withColumn(\"current_timestamp_float\", unix_timestamp(col(\"current_timestamp\"), \"yyyy.MM.dd\"))\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e783cccb-1730-4564-8b37-f9e0e93c01df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_df.withColumn(\"current_timestamp_float\", unix_timestamp(col(\"current_timestamp\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fccfc99-ced6-4bb3-9d13-0d7f38e2ae92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(new_df.withColumn(\"new_date_col\",from_unixtime(col(\"current_timestamp_float\"), \"yyyy-MM-dd\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa552829-08ae-4354-8e3f-de5e9b27ce53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dadb1f8-8281-4d1d-a369-756e4a9ff5d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_df = new_df.withColumn(\"current_date\", to_date(col(\"current_date\"), 'yyyy.MM.dd'))\n",
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f4e1ce9-b926-4ccf-84e5-d34dea087da9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, months_between, add_months, date_add, year, month\n",
    "\n",
    "# df.withColumn('datediff', datediff(col1, col2))\n",
    "# df.withColumn('monthsbetween', months_between(col1, col2))\n",
    "# df.withColumn('addMonths', add_months(col1, 3))\n",
    "# df.withColumn('subMonths', add_months(col2, -5))\n",
    "# df.withColumn('dateAdd', date_add(col1, 10))\n",
    "# # df.withColumn('subDays', date_add(col, -9))\n",
    "# df.withColumn(\"year\", year(col1))\n",
    "# df.withColumn(\"month\", month(col1))\n",
    "# df.withColumn(\"dayofmonth\", dayofmonth(col2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "528516f7-b03b-4a64-a3d7-64006a79fd56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13a8e00b-9e57-44c0-91cd-bcd9385cb297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4336515754456538,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Multiplex_Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
