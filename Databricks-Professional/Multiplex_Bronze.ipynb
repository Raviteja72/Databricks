{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1267ce9-3e0d-4649-a462-4d3aba6ea63d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def path_exists(path):\n",
    "  try:\n",
    "    dbutils.fs.ls(path)\n",
    "    return True\n",
    "  except Exception as e:\n",
    "    if 'java.io.FileNotFoundException' in str(e):\n",
    "      return False\n",
    "    else:\n",
    "      raise\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "class CourseDataset:\n",
    "    def __init__(self, uri, location, checkpoint_path, data_catalog, db_name):\n",
    "        self.uri = uri\n",
    "        self.location = location\n",
    "        self.checkpoint = checkpoint_path\n",
    "        self.catalog_name = data_catalog\n",
    "        self.db_name = db_name\n",
    "    \n",
    "    def download_dataset(self):\n",
    "        source = self.uri\n",
    "        target = self.location\n",
    "        files = dbutils.fs.ls(source)\n",
    "\n",
    "        for f in files:\n",
    "            source_path = f\"{source}/{f.name}\"\n",
    "            target_path = f\"{target}/{f.name}\"\n",
    "            if not path_exists(target_path):\n",
    "                print(f\"Copying {f.name} ...\")\n",
    "                dbutils.fs.cp(source_path, target_path, True)\n",
    "    \n",
    "    \n",
    "    def create_database(self):\n",
    "        spark.sql(f\"USE CATALOG {self.catalog_name}\")\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {self.db_name}\")\n",
    "        spark.sql(f\"USE SCHEMA {self.db_name}\")\n",
    "    \n",
    "    \n",
    "    def clean_up(self):\n",
    "        print(\"Removing Checkpoints ...\")\n",
    "        dbutils.fs.rm(self.checkpoint, True)\n",
    "        print(\"Dropping Database ...\")\n",
    "        spark.sql(f\"DROP SCHEMA IF EXISTS {self.db_name} CASCADE\")\n",
    "        print(\"Removing Dataset ...\")\n",
    "        dbutils.fs.rm(self.location, True)\n",
    "        print(\"Done\")\n",
    "\n",
    "    \n",
    "    def __get_index(self, dir):\n",
    "        try:\n",
    "            files = dbutils.fs.ls(dir)\n",
    "            file = max(f.name for f in files if f.name.endswith('.json'))\n",
    "            index = int(file.rsplit('.', maxsplit=1)[0])\n",
    "        except:\n",
    "            index = 0\n",
    "        return index+1\n",
    "    \n",
    "    \n",
    "    def __load_json_file(self, current_index, streaming_dir, raw_dir):\n",
    "        latest_file = f\"{str(current_index).zfill(2)}.json\"\n",
    "        source = f\"{streaming_dir}/{latest_file}\"\n",
    "        target = f\"{raw_dir}/{latest_file}\"\n",
    "        prefix = streaming_dir.split(\"/\")[-1]\n",
    "        if path_exists(source):\n",
    "            print(f\"Loading {prefix}-{latest_file} file to the bookstore dataset\")\n",
    "            dbutils.fs.cp(source, target)\n",
    "    \n",
    "    \n",
    "    def __load_data(self, max, streaming_dir, raw_dir, all=False):\n",
    "        index = self.__get_index(raw_dir)\n",
    "        if index > max:\n",
    "            print(\"No more data to load\\n\")\n",
    "\n",
    "        elif all == True:\n",
    "            while index <= max:\n",
    "                self.__load_json_file(index, streaming_dir, raw_dir)\n",
    "                index += 1\n",
    "        else:\n",
    "            self.__load_json_file(index, streaming_dir, raw_dir)\n",
    "            index += 1\n",
    "    \n",
    "    def load_new_data(self, num_files = 1):\n",
    "        streaming_dir = f\"{self.location}/kafka-streaming\"\n",
    "        raw_dir = f\"{self.location}/kafka-raw\"\n",
    "        for i in range(num_files):\n",
    "            self.__load_data(10, streaming_dir, raw_dir)\n",
    "        \n",
    "    \n",
    "    def load_books_updates(self):\n",
    "        streaming_dir = f\"{self.location}/books-updates-streaming\"\n",
    "        raw_dir = f\"{self.location}/kafka-raw/books-updates\"\n",
    "        self.__load_data(5, streaming_dir, raw_dir)\n",
    "        \n",
    "    def process_bronze(self):\n",
    "        schema = \"key BINARY, value BINARY, topic STRING, partition LONG, offset LONG, timestamp LONG\"\n",
    "\n",
    "        query = (spark.readStream\n",
    "                            .format(\"cloudFiles\")\n",
    "                            .option(\"cloudFiles.format\", \"json\")\n",
    "                            .schema(schema)\n",
    "                            .load(f\"{self.location}/kafka-raw\")\n",
    "                            .withColumn(\"timestamp\", (F.col(\"timestamp\")/1000).cast(\"timestamp\"))  \n",
    "                            .withColumn(\"year_month\", F.date_format(\"timestamp\", \"yyyy-MM\"))\n",
    "                      .writeStream\n",
    "                          .option(\"checkpointLocation\", f\"{self.checkpoint}/bronze\")\n",
    "                          .option(\"mergeSchema\", True)\n",
    "                          .partitionBy(\"topic\", \"year_month\")\n",
    "                          .trigger(availableNow=True)\n",
    "                          .table(\"bronze\"))\n",
    "\n",
    "        query.awaitTermination()\n",
    "        \n",
    "        \n",
    "    def __upsert_data(self, microBatchDF, batch):\n",
    "        microBatchDF.createOrReplaceTempView(\"orders_microbatch\")\n",
    "    \n",
    "        sql_query = \"\"\"\n",
    "          MERGE INTO orders_silver a\n",
    "          USING orders_microbatch b\n",
    "          ON a.order_id=b.order_id AND a.order_timestamp=b.order_timestamp\n",
    "          WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "\n",
    "        microBatchDF.sparkSession.sql(sql_query)\n",
    "        \n",
    "    def __batch_upsert(self, microBatchDF, batchId):\n",
    "        window = Window.partitionBy(\"customer_id\").orderBy(F.col(\"row_time\").desc())\n",
    "        \n",
    "        (microBatchDF.filter(F.col(\"row_status\").isin([\"insert\", \"update\"]))\n",
    "                     .withColumn(\"rank\", F.rank().over(window))\n",
    "                     .filter(\"rank == 1\")\n",
    "                     .drop(\"rank\")\n",
    "                     .createOrReplaceTempView(\"ranked_updates\"))\n",
    "\n",
    "        query = \"\"\"\n",
    "            MERGE INTO customers_silver c\n",
    "            USING ranked_updates r\n",
    "            ON c.customer_id=r.customer_id\n",
    "                WHEN MATCHED AND c.row_time < r.row_time\n",
    "                  THEN UPDATE SET *\n",
    "                WHEN NOT MATCHED\n",
    "                  THEN INSERT *\n",
    "        \"\"\"\n",
    "\n",
    "        microBatchDF.sparkSession.sql(query)\n",
    "        \n",
    "    \n",
    "    def __type2_upsert(self, microBatchDF, batch):\n",
    "        microBatchDF.createOrReplaceTempView(\"updates\")\n",
    "\n",
    "        sql_query = \"\"\"\n",
    "            MERGE INTO books_silver\n",
    "            USING (\n",
    "                SELECT updates.book_id as merge_key, updates.*\n",
    "                FROM updates\n",
    "\n",
    "                UNION ALL\n",
    "\n",
    "                SELECT NULL as merge_key, updates.*\n",
    "                FROM updates\n",
    "                JOIN books_silver ON updates.book_id = books_silver.book_id\n",
    "                WHERE books_silver.current = true AND updates.price <> books_silver.price\n",
    "              ) staged_updates\n",
    "            ON books_silver.book_id = merge_key \n",
    "            WHEN MATCHED AND books_silver.current = true AND books_silver.price <> staged_updates.price THEN\n",
    "              UPDATE SET current = false, end_date = staged_updates.updated\n",
    "            WHEN NOT MATCHED THEN\n",
    "              INSERT (book_id, title, author, price, current, effective_date, end_date)\n",
    "              VALUES (staged_updates.book_id, staged_updates.title, staged_updates.author, staged_updates.price, true, staged_updates.updated, NULL)\n",
    "        \"\"\"\n",
    "\n",
    "        microBatchDF.sparkSession.sql(sql_query)\n",
    "    \n",
    "    def process_orders_silver(self):\n",
    "        json_schema = \"order_id STRING, order_timestamp Timestamp, customer_id STRING, quantity BIGINT, total BIGINT, books ARRAY<STRUCT<book_id STRING, quantity BIGINT, subtotal BIGINT>>\"\n",
    "        \n",
    "        deduped_df = (spark.readStream\n",
    "                   .table(\"bronze\")\n",
    "                   .filter(\"topic = 'orders'\")\n",
    "                   .select(F.from_json(F.col(\"value\").cast(\"string\"), json_schema).alias(\"v\"))\n",
    "                   .select(\"v.*\")\n",
    "                   .withWatermark(\"order_timestamp\", \"30 seconds\")\n",
    "                   .dropDuplicates([\"order_id\", \"order_timestamp\"]))\n",
    "        \n",
    "        query = (deduped_df.writeStream\n",
    "                   .foreachBatch(self.__upsert_data)\n",
    "                   .outputMode(\"update\")\n",
    "                   .option(\"checkpointLocation\", f\"{self.checkpoint}/orders_silver\")\n",
    "                   .trigger(availableNow=True)\n",
    "                   .start())\n",
    "\n",
    "        query.awaitTermination()\n",
    "\n",
    "        \n",
    "    def process_customers_silver(self):\n",
    "        \n",
    "        schema = \"customer_id STRING, email STRING, first_name STRING, last_name STRING, gender STRING, street STRING, city STRING, country_code STRING, row_status STRING, row_time timestamp\"\n",
    "        \n",
    "        df_country_lookup = spark.read.json(f\"{dataset_bookstore}/country_lookup\")\n",
    "\n",
    "        query = (spark.readStream\n",
    "                          .table(\"bronze\")\n",
    "                          .filter(\"topic = 'customers'\")\n",
    "                          .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "                          .select(\"v.*\")\n",
    "                          .join(F.broadcast(df_country_lookup), F.col(\"country_code\") == F.col(\"code\") , \"inner\")\n",
    "                       .writeStream\n",
    "                          .foreachBatch(self.__batch_upsert)\n",
    "                          .outputMode(\"update\")\n",
    "                          .option(\"checkpointLocation\", f\"{self.checkpoint}/customers_silver\")\n",
    "                          .trigger(availableNow=True)\n",
    "                          .start()\n",
    "                )\n",
    "\n",
    "        query.awaitTermination()\n",
    "    \n",
    "    def process_books_silver(self):\n",
    "        schema = \"book_id STRING, title STRING, author STRING, price DOUBLE, updated TIMESTAMP\"\n",
    "\n",
    "        query = (spark.readStream\n",
    "                        .table(\"bronze\")\n",
    "                        .filter(\"topic = 'books'\")\n",
    "                        .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\"))\n",
    "                        .select(\"v.*\")\n",
    "                     .writeStream\n",
    "                        .foreachBatch(self.__type2_upsert)\n",
    "                        .option(\"checkpointLocation\", f\"{self.checkpoint}/books_silver\")\n",
    "                        .trigger(availableNow=True)\n",
    "                        .start()\n",
    "                )\n",
    "\n",
    "        query.awaitTermination()\n",
    "        \n",
    "    def process_current_books(self):\n",
    "        spark.sql(\"\"\"\n",
    "            CREATE OR REPLACE TABLE current_books\n",
    "            AS SELECT book_id, title, author, price\n",
    "               FROM books_silver\n",
    "               WHERE current IS TRUE\n",
    "        \"\"\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "data_source_uri = \"s3://dalhussein-courses/DE-Pro/datasets/bookstore/v1/\"\n",
    "dataset_bookstore = 'dbfs:/mnt/demo-datasets/DE-Pro/bookstore'\n",
    "spark.conf.set(f\"dataset.bookstore\", dataset_bookstore)\n",
    "checkpoint_path = \"dbfs:/mnt/demo_pro/checkpoints\"\n",
    "data_catalog = 'hive_metastore'\n",
    "db_name = \"bookstore_eng_pro\"\n",
    "\n",
    "bookstore = CourseDataset(data_source_uri, dataset_bookstore, checkpoint_path, data_catalog, db_name)\n",
    "bookstore.download_dataset()\n",
    "bookstore.create_database()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "#bookstore.clean_up()\n",
    "\n",
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "202c8a3b-1323-4079-a2b3-5d26366c4152",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94261aac-97c0-4486-a05f-75efcfa6529d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls dbfs:/mnt/demo-datasets/DE-Pro/bookstore/kafka-raw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1560ecad-e235-44e0-a113-fb5f566664ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp, unix_timestamp, from_unixtime, current_date, date_format, lit, col, date_format, to_date\n",
    "from pyspark.sql.types import StringType, StructType, StructField, IntegerType, LongType, DoubleType\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efca91b5-7edd-4718-8cb4-c1038997560a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw = spark.read.json(\"dbfs:/mnt/demo-datasets/DE-Pro/bookstore/kafka-raw/01.json\")\n",
    "display(df_raw)\n",
    "# value column is the actual data sent as json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bb8d3c8-5215-4dd4-8a07-b67f51b2c7d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae0962ef-c040-413f-b217-a9ea727c8cf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_df = df_raw.withColumn('current_timestamp', current_timestamp()) \\\n",
    "        .withColumn('current_date', current_date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05e53f53-befd-401e-af40-b770918926bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f7ad8b2-60b1-4de9-bcf0-f92f3712ef62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_df = new_df.withColumn(\"current_date\", date_format(col(\"current_date\"), \"yyyy.MM.dd\"))\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "742ec497-eeeb-4b52-9805-5e9b8d3551d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_df = new_df.withColumn(\"current_timestamp_float\", unix_timestamp(col(\"current_timestamp\")))\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e783cccb-1730-4564-8b37-f9e0e93c01df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_df.withColumn(\"current_timestamp_float\", unix_timestamp(col(\"current_timestamp\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fccfc99-ced6-4bb3-9d13-0d7f38e2ae92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(new_df.withColumn(\"new_date_col\",from_unixtime(col(\"current_timestamp_float\"), \"yyyy-MM-dd\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa552829-08ae-4354-8e3f-de5e9b27ce53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dadb1f8-8281-4d1d-a369-756e4a9ff5d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_df = new_df.withColumn(\"current_date\", to_date(col(\"current_date\"), 'yyyy.MM.dd'))\n",
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ccb26ec-b736-49a1-a73e-503e15e2f069",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(new_df.withColumn(\"timestamp_new\", (col(\"timestamp\")/1000).cast(\"timestamp\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f4e1ce9-b926-4ccf-84e5-d34dea087da9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import datediff, months_between, add_months, date_add, year, month\n",
    "from pyspark.sql.functions import *\n",
    "# df.withColumn('datediff', datediff(col1, col2))\n",
    "# df.withColumn('monthsbetween', months_between(col1, col2))\n",
    "# df.withColumn('addMonths', add_months(col1, 3))\n",
    "# df.withColumn('subMonths', add_months(col2, -5))\n",
    "# df.withColumn('dateAdd', date_add(col1, 10))\n",
    "# # df.withColumn('subDays', date_add(col, -9))\n",
    "# df.withColumn(\"year\", year(col1))\n",
    "# df.withColumn(\"month\", month(col1))\n",
    "# df.withColumn(\"dayofmonth\", dayofmonth(col2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b1a4d18-37e8-4936-9dca-fe368d881459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh \n",
    "ls /dbfs/mnt/demo-datasets/DE-Pro-new/bookstore/kafka-raw/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e45459d8-390a-4be3-a587-38d6ab4c9878",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "show tables;\n",
    "-- drop table bronze_table;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f33fe1f-e20f-485c-8533-0a03b6fc13fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_bronze():\n",
    "    schema = \"key BINARY, value BINARY, topic STRING, partition LONG, offset LONG, timestamp LONG\"\n",
    "\n",
    "    query = (spark.readStream\n",
    "                    .format(\"cloudFiles\")\n",
    "                    .option(\"cloudFiles.format\", \"json\")\n",
    "                    .schema(schema)\n",
    "                    .load(\"dbfs:/mnt/demo-datasets/DE-Pro-new/bookstore/kafka-raw\")\n",
    "                    .withColumn(\"timestamp\", (col(\"timestamp\")/1000).cast(\"timestamp\"))\n",
    "                    .withColumn(\"year_month\", date_format(\"timestamp\", \"yyyy-MM\"))\n",
    "                .writeStream\n",
    "                    .option(\"checkpointLocation\", \"dbfs:/mnt/demo_pro/checkpoints/bronze\")\n",
    "                    .option(\"mergeSchema\",\"True\")\n",
    "                    .partitionBy(\"topic\",\"year_month\")\n",
    "                    .trigger(availableNow=True)\n",
    "                    .table(\"bronze_table\")\n",
    "                    )\n",
    "    \n",
    "    query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31fd221e-a449-4f50-a0cd-24bccbf8b166",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "# rm -rf /dbfs/mnt/demo_pro/checkpoints/bronze\n",
    "ls /dbfs/mnt/demo_pro/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b9ce82d-40ac-4e67-aee3-394e32143519",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "process_bronze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99bf7a07-20cf-426e-b3c4-c59d7fc79ed8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- drop table bronze_table;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13a8e00b-9e57-44c0-91cd-bcd9385cb297",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "show tables;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66810392-e70d-4775-8195-601c9e8b410d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from bronze_table;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c98a9128-dc07-480d-9107-6b47c3b2848d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select distinct(topic) from bronze_table;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f6a4f0d-2a62-4e13-9355-895d05f1a107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_df = spark.table(\"bronze_table\")\n",
    "display(bronze_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a80d8bb4-db58-4e0a-a820-53987f9a977c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select cast(key as string), cast(value as string)\n",
    "from bronze_table\n",
    "where topic = \"orders\"\n",
    "limit 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b91e15ef-48f6-41ec-9e66-87630367dc5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select v.*\n",
    "from (\n",
    "  select from_json(cast(value as string), \"order_id string, order_timestamp Timestamp, customer_id string, quantity bigint, total bigint, \n",
    "        books ARRAY<STRUCT<book_id STRING, quantity BIGINT, subtotal BIGINT>>\") as v\n",
    "        from bronze_new_partition\n",
    "        where topic = \"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be78d381-2ef9-47f6-aee2-57d90d75fa1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select v.*\n",
    "from (\n",
    "  select from_json(cast(value as string), schema_of_json('{\"order_id\":\"000000003491\",\"order_timestamp\":\"2021-11-12 09:10:00\",\"customer_id\":\"C00002\",\"quantity\":3,\"total\":99,\"books\":[{\"book_id\":\"B02\",\"quantity\":1,\"subtotal\":28},{\"book_id\":\"B06\",\"quantity\":1,\"subtotal\":22},{\"book_id\":\"B01\",\"quantity\":1,\"subtotal\":49}]}')) v\n",
    "        from bronze_new_partition\n",
    "        where topic = \"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4021242-46bd-4b98-9238-bb9e0fd01674",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace temp view bronze_view as \n",
    "select vals.* from (\n",
    "select from_json(cast(value as STRING), \"customer_id string, order_id string, books ARRAY<STRUCT<book_id string, quantity bigint, subtotal bigint>>, quantity bigint, total bigint, \n",
    "  order_timestamp timestamp\") vals\n",
    "  from bronze_new_partition\n",
    "  where topic = \"orders\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7664f313-7688-4262-bee3-0c0941462e88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from bronze_view;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36e1e281-1dd7-4bfa-b85b-cd631ba2a500",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop view bronze_view;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eb9326f-4a64-4c6d-ae6a-32d413112af6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.readStream\n",
    "      .table(\"bronze_new_partition\")\n",
    "      .createOrReplaceTempView(\"bronze_view\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e84bf69-0dd6-476f-bf55-23c6cf1aa104",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from bronze_view;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c30947c-7105-4f6f-a0bd-aea00ba25572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create or replace temporary view silver_tmp_view as \n",
    "select vals.* from (\n",
    "  select from_json(cast(value as STRING), \"customer_id string, order_id string, books ARRAY<STRUCT<book_id string, quantity bigint, subtotal bigint>>, quantity bigint, total bigint, order_timestamp timestamp\")as vals\n",
    "  from bronze_view\n",
    "  where topic = 'orders'\n",
    "); \n",
    "\n",
    "-- select from_json(cast(value as STRING), \"customer_id string, order_id string, books ARRAY<STRUCT<book_id string, quantity bigint, subtotal bigint>>, quantity bigint, total bigint, \n",
    "--   order_timestamp timestamp\") vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dac1c4ce-3c40-45e2-80d9-e5ad9cec58eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from silver_tmp_view;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6827eed-ec0e-47a9-8e50-78d9a7f8c5cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = spark.table(\"silver_tmp_view\") \\\n",
    "        .writeStream \\\n",
    "        .option(\"checkpointlocation\",\"/dbfs:/mnt/demo_pro_new/checkpoints/silver\") \\\n",
    "        .trigger(availableNow=True) \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .table(\"orders_silver\")\n",
    "  \n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8978e907-511a-4671-ad2c-7e8187e263ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7417e9ca-c0a9-4c93-a5fb-d586e956bbf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "json_schema = \"customer_id string, order_id string, books ARRAY<STRUCT<book_id string, quantity bigint, subtotal bigint>>, quantity bigint, total bigint, order_timestamp timestamp\"\n",
    "\n",
    "# from_json() -> it will parse the JSON string and create a row with the parsed data\n",
    "\n",
    "query = (spark.readStream\n",
    "                .table(\"bronze_table\")\n",
    "                .filter(\"topic = 'orders'\")\n",
    "                .select(from_json(col(\"value\").cast(\"string\"), json_schema).alias(\"v\"))\n",
    "                .select(\"v.*\")\n",
    "            .writeStream\n",
    "                .option(\"checkpointLocation\", \"dbfs:/mnt/demo_pro/checkpoint/silver/\")\n",
    "                .trigger(availableNow=True)\n",
    "                .table(\"orders_ultimatum\"))\n",
    "query.awaitTermination()\n",
    "\n",
    "\n",
    "# query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38aa545a-e139-4482-a887-406435696f99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from orders_ultimatum;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e444f659-4807-4460-a9b9-7b00b2929321",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select b.customer_id, b.order_id, b.order_quantity, b.order_total, b.exploded_books.book_id as book_id, b.exploded_books.quantity as book_quantity, b.exploded_books.subtotal as books_subtotal\n",
    "from ( \n",
    "select customer_id, order_id, explode(books) as exploded_books, quantity as order_quantity, total as order_total, order_timestamp from orders_ultimatum) b\n",
    "order by b.order_timestamp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5b5dfed-440a-4f69-b3c3-f70425e6ca0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "alter table orders_ultimatum add constraint timestamp_within_range check(order_timestamp >= '2020-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e01ad6f-0a96-4f67-af5f-68cb7de576e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "describe extended orders_ultimatum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "995092b4-6e7d-492e-b3b7-d2114667f80a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "alter table orders_ultimatum add constraint check_quantity check(quantity > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7c4358d-2dbb-4ba3-9237-da637bca960b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_schema = \"customer_id string, order_id string, books ARRAY<STRUCT<book_id string, quantity bigint, subtotal bigint>>, quantity bigint, total bigint, order_timestamp timestamp\"\n",
    "\n",
    "# from_json() -> it will parse the JSON string and create a row with the parsed data\n",
    "\n",
    "query = (spark.readStream\n",
    "                .table(\"bronze_table\")\n",
    "                .filter(\"topic = 'orders'\")\n",
    "                .select(from_json(col(\"value\").cast(\"string\"), json_schema).alias(\"v\"))\n",
    "                .select(\"v.*\")\n",
    "                .where(\"v.quantity > 0\")\n",
    "            .writeStream\n",
    "                .option(\"checkpointLocation\", \"dbfs:/mnt/demo_pro/checkpoint/silver-updated\")\n",
    "                .trigger(availableNow=True)\n",
    "                .table(\"orders_silver\"))\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4593c258-d2e8-404b-8aba-4479eaf4aecd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "# rm -rf /dbfs/mnt/demo_pro/checkpoint/remove-duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54342905-d287-473d-a47a-a37b6f65d464",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- drop table orders_silver;\n",
    "select * from orders_silver;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ce1153e-80d7-495c-8cd8-d14935213287",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "alter table orders_ultimatum drop constraint timestamp_within_range;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9eafec85-d9c8-4d2c-b54b-e87baa37daf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "describe extended orders_ultimatum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0d38706-807a-432e-8ee7-9b83e0143a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read \\\n",
    "    .table(\"bronze_table\") \\\n",
    "    .filter(\"topic = 'orders'\") \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f8321cf-3f1b-4c4a-9c4f-d23f063bc6f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_schema = \"customer_id string, order_id string, books ARRAY<STRUCT<book_id string, quantity bigint, subtotal bigint>>, quantity bigint, total bigint, order_timestamp timestamp\"\n",
    "\n",
    "batch_total = (spark.read\n",
    "                    .table(\"bronze_table\")\n",
    "                    .filter(\"topic = 'orders'\")\n",
    "                    .select(from_json(col(\"value\").cast(\"string\"), json_schema).alias(\"v\"))\n",
    "                    .select(\"v.*\")\n",
    "                    .where(\"quantity > 0\")\n",
    "                    .dropDuplicates([\"order_id\",\"order_timestamp\"])\n",
    "                    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66e0b66e-f784-453f-8e3c-ce95c9d45aae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "batch_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3812ab3-abc4-48dd-bd21-5243d836edae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "truncate table orders_silver;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7faa310b-501c-4bc3-b9c5-c600a0f41369",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_schema = \"customer_id string, order_id string, books ARRAY<STRUCT<book_id string, quantity bigint, subtotal bigint>>, quantity bigint, total bigint, order_timestamp timestamp\"\n",
    "\n",
    "deduped_df = spark.readStream \\\n",
    "                .table(\"bronze_table\") \\\n",
    "                .filter(\"topic = 'orders'\") \\\n",
    "                .select(from_json(col(\"value\").cast(\"string\"), json_schema).alias(\"v\")) \\\n",
    "                .select(\"v.*\")  \\\n",
    "                .where(\"quantity > 0\") \\\n",
    "                .withWatermark(\"order_timestamp\", \"30 seconds\") \\\n",
    "                .dropDuplicates([\"order_id\", \"order_timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4465a275-e6e0-4c20-a327-7fcb6b1e02b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(deduped_df)\n",
    "# It will the streaming ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e5dba5e-1692-4ae6-86eb-a8f7848d1b37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def upsert_data(microBatchDF, batch):\n",
    "    microBatchDF.createOrReplaceTempView(\"orders_microbatch\")\n",
    "\n",
    "    sql_query = \"\"\"\n",
    "        MERGE INTO orders_silver a\n",
    "        using orders_microbatch b\n",
    "        on a.order_id = b.order_id and a.order_timestamp = b.order_timestamp\n",
    "        when not matched then insert *\n",
    "    \"\"\"\n",
    "    # spark.sql(sql_query)\n",
    "    microBatchDF.sparkSession.sql(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9810917c-2c19-40f2-8d9f-69065cb06f26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = (deduped_df.writeStream \n",
    "                    .foreachBatch(upsert_data)\n",
    "                    .option(\"checkpointLocation\",\"dbfs:/mnt/demo_pro/checkpoint/\")\n",
    "                    .trigger(availableNow = True)\n",
    "                    .start())\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3f01d3c-6efc-4e82-b3d8-8f378cb0a20e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select count(*) from orders_silver;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7123767c-d25f-4d27-aa40-592d790c1fb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "streaming_total = spark.read.table(\"orders_silver\").count()\n",
    "# batch_total = spark.read.table(\"bronze_table\").count()\n",
    "print(f\"batch total: {batch_total}\")\n",
    "print(f\"Streaming total: {streaming_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd502486-91e1-4915-8115-b6261e5e1b38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create table if not exists books_silver\n",
    "(book_id string, title string, author string, price double, current boolean, effective_date timestamp, end_date timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e00df42-480e-4c58-87b3-521396623489",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def type1_upsert(microBatchDF, batch):\n",
    "  microBatchDF.createOrReplaceTempView(\"updates\")\n",
    "\n",
    "  sql_query = \"\"\"\n",
    "  merge into books_silver\n",
    "  using (\n",
    "    select updates.book_id as merge_key, updates.*\n",
    "    from updates\n",
    "\n",
    "    union all\n",
    "\n",
    "    select updates.book_id as NULL, updates.*\n",
    "    from books_silver\n",
    "    join books_silver on updates.book_id = books_silver.book_id\n",
    "    where books_silver.current = true and updates.price <> books_silver.price\n",
    "  ) staged_updates\n",
    "  on books_silver.book_id = merge_key\n",
    "  when matched and books_silver.current = true and books_silver.price <> staged_updates.price then\n",
    "    update set current = false, end_date = staged_updates.updated\n",
    "  when not matched then\n",
    "    insert (book_id, title, author, price, current, effective_date, end_date)\n",
    "    values(staged_updates.book_id, staged_updates.title, staged_updates.author, staged_updates.price, true, staged_updates.updated, NULL)\n",
    "  \"\"\"\n",
    "\n",
    "  microBatchDF.sparkSession.sql(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df6bdf74-ea13-42b7-96aa-a8c221f80598",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_schema = \"customer_id string, order_id string, books ARRAY<STRUCT<book_id string, quantity bigint, subtotal bigint>>, quantity bigint, total bigint, order_timestamp timestamp\"\n",
    "\n",
    "querydf = spark.readStream \\\n",
    "                .table(\"bronze_table\") \\\n",
    "                .filter(\"topic = 'books'\") \\\n",
    "                .select(from_json(col(\"value\").cast(\"string\"), json_schema).alias(\"v\")) \\\n",
    "                .select(\"v.*\")  \\\n",
    "              .writeStream \\\n",
    "                .foreachBatch(type1_upsert) \\\n",
    "                .option(\"checkpointLocation\", \"dbfs:/mnt/demo_pro/checkpoints/books_silver\") \\\n",
    "                .trigger(availableNow=True) \\\n",
    "                .start()\n",
    "  \n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5658ebe1-445c-4402-8860-364db9b0e73b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21cd4072-6c7f-4981-bb42-edc160ed1cf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8f05640-2c01-4e84-8f60-bf20f9f5efa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "439ecbc9-19ea-4051-a98a-83fbafd723e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d423ef0-e3b0-4cd5-8520-da037ac178c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbcf26c3-eff2-447e-873b-e6d3050a3923",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59108cdb-e013-48c4-8973-aed5ab31c6db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2dbf206-50fe-46df-816c-7749196bb21f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1be5365-5f2f-43e2-ba58-34e9e4bbf40d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3202a44b-1637-498d-b1ac-b983c12554bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a90744d6-7b5c-4834-9c70-75fd612e0889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48a3c126-293d-412f-a191-0bf334f5a600",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efdb2f20-7c44-4deb-8e85-99b5a1bfc616",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eef4bf17-bc0c-4768-964b-51e1dc154f6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "427c8cb3-e74c-4d51-9577-31010d57e57c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51d90979-fb45-4929-ad9d-cbcb0f33f7de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2205f99b-7669-4f9f-8e08-dc7f4c40dc90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "803fea64-84e8-448d-a43e-fa71d022baf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e414775-6135-4f97-996f-ed726c4b9d49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a584b566-edc9-46fb-aa96-071323b16e31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3910365037924736,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Multiplex_Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
